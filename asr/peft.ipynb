{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf4ef3b-844c-4fd4-8c5b-2e7fca54d5f0",
   "metadata": {},
   "source": [
    "https://github.com/Vaibhavs10/fast-whisper-finetuning/blob/main/Whisper_w_PEFT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce873fc0-9ba4-42ee-a830-7ea9b2489ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes accelerate \n",
    "!pip install -q git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c9f8c9-7a35-423f-b279-2beaeb318980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>audio</th>\n",
       "      <th>transcript</th>\n",
       "      <th>b64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>audio_0.wav</td>\n",
       "      <td>Turret, prepare to deploy electromagnetic puls...</td>\n",
       "      <td>UklGRgB3BABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>audio_1.wav</td>\n",
       "      <td>Engage yellow drone with surface-to-air missil...</td>\n",
       "      <td>UklGRubrAgBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>audio_2.wav</td>\n",
       "      <td>Control to turrets, deploy electromagnetic pul...</td>\n",
       "      <td>UklGRgCXBwBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>audio_3.wav</td>\n",
       "      <td>Alfa, Echo, Mike Papa, deploy EMP tool heading...</td>\n",
       "      <td>UklGRiRgBABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>audio_4.wav</td>\n",
       "      <td>Engage the grey, black, and green fighter plan...</td>\n",
       "      <td>UklGRvaYAwBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   key        audio                                         transcript  \\\n",
       "0    0  audio_0.wav  Turret, prepare to deploy electromagnetic puls...   \n",
       "1    1  audio_1.wav  Engage yellow drone with surface-to-air missil...   \n",
       "2    2  audio_2.wav  Control to turrets, deploy electromagnetic pul...   \n",
       "3    3  audio_3.wav  Alfa, Echo, Mike Papa, deploy EMP tool heading...   \n",
       "4    4  audio_4.wav  Engage the grey, black, and green fighter plan...   \n",
       "\n",
       "                                                 b64  \n",
       "0  UklGRgB3BABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...  \n",
       "1  UklGRubrAgBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...  \n",
       "2  UklGRgCXBwBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...  \n",
       "3  UklGRiRgBABXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...  \n",
       "4  UklGRvaYAwBXQVZFZm10IBAAAAABAAEAgD4AAAB9AAACAB...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATAFRAME_PATH = './tmp/asr_dataframe.pkl'\n",
    "FILE_PATH = \"../../advanced\"\n",
    "\n",
    "if os.path.exists(DATAFRAME_PATH):\n",
    "    df = pd.read_pickle(DATAFRAME_PATH) \n",
    "else:\n",
    "    df = read_data(FILE_PATH, DATAFRAME_PATH)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236d9245-acaf-4afa-af69-460eae1bf0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Audio, Dataset\n",
    "\n",
    "directory_prefix = \"../../advanced/audio/\"\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "def df_to_dataset(df):\n",
    "    df['audio'] = df['audio'].apply(lambda x: directory_prefix + x)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c6eeac-4054-4434-ab7b-3b9a87f2dc3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['key', 'audio', 'transcript', 'b64', '__index_level_0__'],\n",
      "        num_rows: 2800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['key', 'audio', 'transcript', 'b64', '__index_level_0__'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the DataFrame into train, test, and validation DataFrames\n",
    "train_df = df.sample(frac=0.8, random_state=42)  # 80% for training\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Convert DataFrames to Datasets\n",
    "train_dataset = df_to_dataset(train_df)\n",
    "test_dataset = df_to_dataset(test_df)\n",
    "\n",
    "# Create a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset,\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8405a699-2f61-44ac-b7d8-d653979474c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = dataset_dict.remove_columns(\n",
    "    [\"b64\", \"__index_level_0__\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540f80b5-aa84-4e80-ad56-e10cfd94a052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "task = \"transcribe\"\n",
    "\n",
    "language = \"English\"\n",
    "language_abbr = \"en\" # Short hand code for the language we want to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4b84b4-e300-46e8-8e66-3492c2dec23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ff097e3-e3ac-47dc-b840-129cd75081c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad25b4-5675-411b-98ca-7f7c26a0bee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb97ef-3c10-4a6f-9184-a87cf8bd8a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset_dict['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04321a43-b099-4a3f-aeb7-82baec8c0891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcript\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "698c6609-6ac7-466d-a55f-21b8d2ca6480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ab03527723481588b853d6d98302a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fe12fcf8d84436b1fcc6c73658d217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_features', 'labels'],\n",
      "    num_rows: 2800\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "train_dict = dataset_dict.map(prepare_dataset, remove_columns=dataset_dict.column_names[\"train\"], num_proc=1)\n",
    "\n",
    "DATASETDICT_PATH = './tmp/asr_datasetdict.pkl'\n",
    "with open(DATASETDICT_PATH, \"wb\") as f:\n",
    "    pickle.dump(train_dict, f)\n",
    "\n",
    "print(train_dict)\n",
    "print(train_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08260795-caf3-49c6-94db-8e56a06ddb4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c125b03-7182-4603-a2ec-53d9317d988c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccbc7a-3278-47d5-86be-0e05cdd27e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2df82d59-64e3-48a6-8165-ccc7461a03dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.37.0)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install accelerate \n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fb8e55c-30cf-4680-91b5-151561fc0bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.37.0\n",
      "Summary: 8-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Name: accelerate\n",
      "Version: 0.30.1\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft\n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes\n",
    "!pip show accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff47a3-adc5-4b60-9cfc-f46eea5f651c",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig, WhisperConfig\n",
    "\n",
    "config = WhisperConfig.from_pretrained(model_name_or_path, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n",
    "\n",
    "# Initialize model\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703ffb2-ad78-449a-9b04-fc6336f5d0d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c335fd69-36af-474d-902d-ac984b3e68ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_inputs_require_grad(module, input, output):\n",
    "    output.requires_grad_(True)\n",
    "\n",
    "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3d36f-e5b3-4b96-8fa3-13f82e0e0e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc00119-7736-4a0a-9269-0d0774ac22ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./helps-peft\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-3,\n",
    "    warmup_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_checkpointing=True,\n",
    "    # fp16=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    generation_max_length=128,\n",
    "    save_steps=100,\n",
    "    logging_steps=5,\n",
    "    max_steps=5, # only for testing purposes, remove this from your final run :)\n",
    "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
    "    label_names=[\"labels\"],  # same reason as above\n",
    "    push_to_hub=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4a7ec-befd-455c-8efe-2f5d67970e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "# This callback helps to save only the adapter weights and remove the base model weights.\n",
    "class SavePeftModelCallback(TrainerCallback):\n",
    "    def on_save(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
    "\n",
    "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
    "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
    "\n",
    "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
    "        if os.path.exists(pytorch_model_path):\n",
    "            os.remove(pytorch_model_path)\n",
    "        return control\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dict[\"train\"],\n",
    "    eval_dataset=train_dict[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[SavePeftModelCallback],\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2391b81-9c09-4ef5-be4b-54b58075c676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0a2af5-9c10-4fca-b0b5-181897060d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 2800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 700\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from datasets import Audio, Dataset, DatasetDict\n",
    "\n",
    "DATASETDICT_PATH = './tmp/asr_datasetdict.pkl'\n",
    "    \n",
    "with open(DATASETDICT_PATH, \"rb\") as f:\n",
    "    dataset_dict = pickle.load(f)\n",
    "    print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cda291-cac1-4eff-bcfc-579b7f7bc5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 18 12:36:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7d935b-0066-4cf3-9622-2c0048dffe5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Check the number of GPUs\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# Get the name of the GPU\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a346d-b433-4b22-81ff-c7ef9bb06e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
